# 深度思考笔记

大模型实现深度思考的几种方式

## supervised chain of thought

通过在prompt中添加强制的思考过程，让模型触发思考；

> 如何避免想太多

chain of draft: prompt限制输出的步骤的数量，以及每个step的word；

## 给模型推理流程

让语言模型同一个问题不断的去尝试；

majority vote: 选择生成答案中出现次数最多的答案；
best-of-n:
beam search: 分步骤的解题，每一步都过滤掉错误的尝试，每一步选择得分最高的；
monte carlo tree search: 

> 如何避免想太多

控制尝试的数量，以及树的结构，来减少reasoning的step；

## 教模型推理过程

stream of search

imitation learning

用语言模型生成的多次分步骤的思考过程，采用深度优先的方式，搜寻一条得到正确答案的路径，当遇到错误step时就回到上一个节点，直到达到正确结果。

用这条既包含正确也包含错误的step推理过程作为训练资料去交给模型训练，这样通过后训练就可以让模型具有深度思考的能力。

> 如何避免想太多

生成的正确答案和推理过程的各条记录，选择推理过程最短的作为训练资料。

## 以结果为导向学习推理

RL：强化学习，对一个强模型有效，生成正确答案给激励，强化他深度思考的能力，本身就具有思考的能力；

不关心推论过程的正确，只关心结果，只有结果对，就给激励，通过RL的方式强化基础模型思考的能力，deepseekR1采用RL的方式训练；

> 如何避免想太多

RL时加上推理长度的reward，同样产生了正确答案，推理越短reward越高。